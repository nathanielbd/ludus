% more metrics, other optimization methods

The genre of auto battlers is very young, and our research only explores a narrow slice of questions in this new area. We discuss problems that extend our current work.

\subsection{Other Auto Battler Features}

% draft phase of auto battlers

Unlike our game, some auto battlers have
players iteratively build their deck between battles by purchasing
cards from an array of options. While our experiments were concerned with fixed decks of size 3, 
{\sc Ludus}'s ability to quantify, qualify, and optimize balance for other deck-building rules %with more or fewer cards
remains to be assessed. We previously only considered
the case where any card may be replaced with any other card to create a new deck. However, 
the purchasing value of cards is another dimension of their design; %may be different and
future work should consider comparing decks
that players can build after the same number of battles and purchasing phases.

% cards with multiple mechanics

In many auto battlers, cards have multiple special mechanics that each have variable parameters.
Our work only considers cards with a single mechanic, but this should be extended to optimize design for such cards.

% non-determinism/random effects? is this in other games besides heartstone battlegrouds?

\subsection{Simulation Data}

All metrics we present are based off of the average win rates of decks collected from the simulator. 
This does not take into account individual deck-versus-deck win rates, which are always 0 or 1 since
our auto battler game is deterministic. One could represent these relations via a
\textit{domination graph} with decks
as nodes and a directed edge to the deck that wins the head-to-head matchup \cite{gkmmmf_eaai21},
game matrices with each deck as a strategy, and more.  The size of these data structures
scale quadratically with the number of decks just like the round-robin tournament, which means more approximation algorithms would be necessary in practice.
Further investigation is required to determine whether perturbations of card parameters causing a degradation
in metagame health correspond to any changes in these representations.  There are also game design factors to consider besides win rates, such as game durations and intensity levels.
\if{false}
construct a \textit{domination graph} with decks
as nodes and a directed edge to the deck that wins the head-to-head matchup. The size of the domination
graph problematically scales with the square of the number of decks just like the round-robin tournament,
so another sampling method would be necessary in practice. Further investigation is required to determine if perturbation of card parameters causing a degradation
in metagame health corresponds to any phase transition in a domination graph.
\fi

% We don't consider deck vs deck win rates, only average win rates

% mention our original pareto idea?

\subsection{Alternative Optimization Methods}

Our choice of a genetic algorithm for optimization is somewhat arbitrary. Genetic algorithms can conveniently 
handle the familiar integer values of card parameters, unlike other optimization methods. Gradient- and Hessian-based
methods in particular are difficult to apply to this problem due to its discrete nature and our simulator not being
differentiable. Had they been applicable, they would have solved an unmet need of %qualifying
validating the stability of optimization
results with respect to the variable parameters.

\subsection{Human Playtesting}

Human playtesting is still required to determine the health of the metagame as we have not verified to what extent our metrics
correspond to what humans deem as a healthy metagame. Designers
with a historically healthy metagame can use those parameters to generate reference win rate distributions 
over decks and cards using our {\sc Ludus} framework. These distributions can then shape 
new metrics and imply their ideal values for parity. Human playtesting feedback could also motivate other metrics based on the gameplay experience,
such as the number of turns. %in each match.
While this work is an example of how designers can readily apply the {\sc Ludus} framework,
researching human playtesting-validated results marks an important avenue of 
future work.

% dumb idea as the simulation is not differentiable
% One benefit of using a different optimization method is
% providing sensitivity analysis; some algorithms, like the Broyden--Fletcher--Goldfarb--Shanno (BFGS) algorithm, 
% approximate the Hessian of the optimization function, which could be useful in finding the sensitivity of the
% metagame health with respect to card parameters. Further consideration is needed to develop a version of these
% types of optimization 
