% more metrics, other optimization methods

The genre of auto battlers is very new, as such our research only explores a narrow slice of questions in this area. We discuss problems that extend our current work.

\subsection{Other auto battler features}

% draft phase of auto battlers

Unlike our implementation, in some auto battlers,
players iteratively build their deck between battles by purchasing
cards from an array of options. While our experiments were concerned with decks of size 3, 
the ability of our framework to quantify, qualify, and optimize balance for decks with 
more or fewer cards remains to be assessed. We previously only considered
the case where any card may be replaced with any other card to create a new deck. However, 
the purchasing value of cards may be different and future work should consider comparing decks
which may be built after the same number of battles and purchasing phases.

% cards with multiple mechanics

In many auto battlers, cards have multiple mechanics which may or may not have special parameters.
Our work only considers cards with a single mechanic, but should be extended to include such cards.

% non-determinism/random effects? is this in other games besides heartstone battlegrouds?

\subsection{Simulation data}

All metrics we present are based off of the average win rates of decks collected from the simulator. 
This does not take into account individual deck-versus-deck win rates, which are always 0 or 1 since
our auto battler game is deterministic. One could construct a \textit{domination graph} with decks
as nodes and a directed edge to the deck that wins the head-to-head matchup. The size of the domination
graph problematically scales with the square of the number of decks just like the round-robin tournament,
so another sampling method would be necessary in practice. Further investigation is required to determine if perturbation of card parameters causing a degradation
in metagame health corresponds to any phase transition in a domination graph.

% We don't consider deck vs deck win rates, only average win rates

% mention our original pareto idea?

\subsection{Alternative optimization methods}

Our choice of a genetic algorithm for optimization is somewhat arbitrary. Genetic algorithms can conveniently 
handle the familiar integer values of card parameters, unlike other optimization methods. Gradient- and Hessian-based
methods in particular are difficult to apply to this problem due to its discrete nature and our simulator not being
differentiable. Had they been applicable, they would have solved an unmet need of qualifing the stability of optimization
results with respect to the variable parameters.

\subsection{Human playtesting}

Human playtesting is still required to determine the health of the metagame, as we do not know if our metrics
correspond to what humans deem as a healthy metagame. Designers
with a historically healthy metagame can use those parameters to generate reference win rate 
over decks and win rate over cards distributions using our framework. These distributions could then shape 
what new metrics and their ideal values may be for parity. Human playtesting could also motivate other metrics pertaining
to the number of turns in each match. While this is an example of how the {\sc Ludus} framework
could be readily applied by designers, researchers finding human playtesting-validated results marks an important avenue of 
future work.

% dumb idea as the simulation is not differentiable
% One benefit of using a different optimization method is
% providing sensitivity analysis; some algorithms, like the Broyden--Fletcher--Goldfarb--Shanno (BFGS) algorithm, 
% approximate the Hessian of the optimization function, which could be useful in finding the sensitivity of the
% metagame health with respect to card parameters. Further consideration is needed to develop a version of these
% types of optimization 